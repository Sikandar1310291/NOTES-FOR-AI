--------------------------- Embeddings with RNN — Code Notes---------------------------------
                            1) Pipeline :

             Text → Tokenizer → Embedding → RNN → Output

                            2) Basic Architecture :

                    Input (word IDs)
                           ↓
                    Embedding Layer
                           ↓
                    RNN / LSTM / GRU
                           ↓
                      Dense Layer
                           ↓
                      Prediction

                            3) Tokenization & Padding :

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Tokenizer
tokenizer = Tokenizer()
tokenizer.fit_on_texts(sentences)

# Convert text to sequences
sequences = tokenizer.texts_to_sequences(sentences)

# Vocabulary size
vocab_size = len(tokenizer.word_index) + 1

# list of tokenized sequences (lists of integers)

import numpy as np
max_len = max(len(seq) for seq in sequence)
print("Maximum review length:", max_len)


# Padding
max_len = 5
X = pad_sequences(sequences, maxlen=max_len)

print("Sequences:\n", X)
print("Vocabulary Size:", vocab_size)

                             4) Embedding + RNN Model (Core Part) :

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, SimpleRNN, Dense
embedding_dim = 50

model = Sequential([
    Embedding(
        input_dim=vocab_size,
        output_dim=embedding_dim,
        input_length=max_len
    ),
    SimpleRNN(64, activation='tanh'),
    Dense(1, activation='sigmoid')
])

                            5) Compile The Model :

model.compile(
    optimizer='adam',
    loss='binary_crossentropy',
    metrics=['accuracy']
)

model.summary()

